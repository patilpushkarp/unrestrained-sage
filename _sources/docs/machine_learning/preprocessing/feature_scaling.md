# Feature Tuning I - Feature Scaling

## Introduction to feature scaling

A dataset may have hundreds or thoudands of features and they may vary widely in terms of units used in calculating them, source of origin and data range in which they lie. Feature scaling can improve performance for certain machine learning algorithms (algorithms using gradient descent and distance-based metrics) but may work the other way around for few others (tree based algorithms which work by splitting data on single feature).  
There are two widely used feature scaling methods -  
1. Normalization  
2. Standardization  

## Need for feature scaling

- Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Having features on a similar scale help the gradient descent converge more quickly towards the minima.  
- Distance based algorithms such as KNN, K-means, and SVM are affected by the range of data. 


```python
import pandas as pd
from IPython.display import Markdown, display

# Load training and testing data
train_df_with_target = pd.read_csv("datasets/cleaned_train.csv")
test_df = pd.read_csv("datasets/cleaned_test.csv")

# Separate out the target variable as it should not be normalized or standardized
train_df = train_df_with_target.drop('TARGET', axis=1)
```

## Normalization

### Definition

Normalization is the scaling technique in which values are shifted and scaled in such a way that all the values lie between 0 and 1. It is also known as Min-Max scaling.

### Formula

$$
X' = \frac{X - X_{min}}{X_{max} - X_{min}}
$$ 

where, $X_{min}$ is the minimum value of the feature and $X_{max}$ is the maximum value of the feature.  

Scikit-learn package has an implementation for normalizing the data and the name of the class is MinMaxScaler


```python
# Import MinMaxScaler class which will be utilized for normalization
from sklearn.preprocessing import MinMaxScaler

# Instatiate an instance of MinMaxScaler class
normalizer = MinMaxScaler().fit(train_df)

# Normalize training data
normalized_train_df = normalizer.transform(train_df)

# Normalize testing data
normalized_test_df = normalizer.transform(test_df)
```


```python
display(Markdown(pd.DataFrame(normalized_train_df).head().to_markdown()))
```


|    |           0 |           1 |        2 |    3 |   4 |         5 |   6 |   7 |   8 |   9 |   10 |   11 |   12 |   13 |   14 |   15 |   16 |   17 |   18 |   19 |   20 |   21 |   22 |   23 |   24 |   25 |   26 |   27 |   28 |   29 |   30 |   31 |   32 |   33 |   34 |   35 |   36 |   37 |   38 |   39 |   40 |   41 |   42 |       43 |   44 |   45 |   46 |   47 |   48 |   49 |   50 |   51 |   52 |        53 |   54 |   55 |   56 |         57 |        58 |        59 |   60 |   61 |   62 |   63 |   64 |   65 |   66 |        67 |   68 |   69 |   70 |       71 |    72 |    73 |    74 |    75 |   76 |    77 |   78 |    79 |   80 |    81 |   82 |    83 |   84 |   85 |          86 |   87 |   88 |   89 |   90 |   91 |   92 |   93 |   94 |   95 |   96 |       97 |   98 |   99 |   100 |   101 |   102 |   103 |   104 |   105 |   106 |   107 |   108 |   109 |   110 |   111 |       112 |   113 |    114 |       115 |   116 |   117 |   118 |   119 |       120 |       121 |       122 |   123 |   124 |   125 |         126 |         127 |        128 |   129 |   130 |         131 |   132 |   133 |   134 |   135 |   136 |   137 |   138 |        139 |
|---:|------------:|------------:|---------:|-----:|----:|----------:|----:|----:|----:|----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|---------:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|----------:|-----:|-----:|-----:|-----------:|----------:|----------:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|----------:|-----:|-----:|-----:|---------:|------:|------:|------:|------:|-----:|------:|-----:|------:|-----:|------:|-----:|------:|-----:|-----:|------------:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|---------:|-----:|-----:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|----------:|------:|-------:|----------:|------:|------:|------:|------:|----------:|----------:|----------:|------:|------:|------:|------------:|------------:|-----------:|------:|------:|------------:|------:|------:|------:|------:|------:|------:|------:|-----------:|
|  0 | 0           | 0           | 0.999764 | 0.18 |   0 | 0         |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 | 0        |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 | 0         |    0 |    0 |    0 | 0.00465382 | 0.0201735 | 0         |    0 |    0 |    0 |    0 |    0 |    0 |    0 | 0         |    0 |    0 |    0 | 1        | 1e-10 | 1e-10 | 1e-10 | 1e-10 |    0 | 1e-10 |    0 | 1e-10 |    0 | 1e-10 |    0 | 1e-10 |    0 |    0 | 0           |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 | 0        |    0 |    0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 | 0         |     0 | 0      | 0         |     0 |     0 |     0 |     0 | 0         | 0         | 0         |     0 |     0 |     0 | 0.000158039 | 5.21283e-06 | 0.00124189 |     0 |     0 | 0           |     0 |     0 |     0 |     0 |     0 |     0 |     0 | 0.00154526 |
|  1 | 1.31546e-05 | 1.3172e-05  | 0.999764 | 0.29 |   0 | 0         |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 | 0.142857 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 | 0         |    0 |    0 |    0 | 0.00465382 | 0.0201735 | 0         |    0 |    0 |    0 |    0 |    0 |    0 |    0 | 0         |    0 |    0 |    0 | 0.030303 | 0     | 1e-10 | 1e-10 | 1e-10 |    0 | 1e-10 |    0 | 1e-10 |    0 | 1e-10 |    0 | 1e-10 |    0 |    0 | 0.000357143 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 | 0        |    0 |    0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 | 0         |     0 | 0      | 0         |     0 |     0 |     0 |     0 | 0         | 0         | 0         |     0 |     0 |     0 | 0.000158039 | 6.28458e-05 | 0.00124189 |     0 |     0 | 0.000400933 |     0 |     0 |     0 |     0 |     0 |     0 |     0 | 0.0020025  |
|  2 | 2.63092e-05 | 1.9758e-05  | 0.999764 | 0.18 |   0 | 0         |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    1 |    1 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 | 0.142857 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 | 0         |    0 |    0 |    0 | 0.00465864 | 0.0201735 | 0         |    0 |    0 |    0 |    0 |    0 |    0 |    0 | 0         |    0 |    0 |    0 | 1        | 1e-10 | 1e-10 | 1e-10 | 1e-10 |    0 | 1e-10 |    0 | 1e-10 |    0 | 1e-10 |    0 | 1e-10 |    0 |    0 | 0           |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 | 0        |    0 |    0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 | 0         |     0 | 0      | 0         |     0 |     0 |     0 |     0 | 0         | 0         | 0         |     0 |     0 |     0 | 0.000161733 | 5.32954e-06 | 0.00124189 |     0 |     0 | 0           |     0 |     0 |     0 |     0 |     0 |     0 |     0 | 0.00282212 |
|  3 | 3.94638e-05 | 4.61021e-05 | 0.999764 | 0.32 |   0 | 0.0151303 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    1 |    1 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 |    1 |    1 |    0 |    0 |    0 |    0 | 0.428571 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 | 0.0571429 |    0 |    0 |    0 | 0.00476731 | 0.0201735 | 0         |    0 |    0 |    0 |    0 |    0 |    0 |    0 | 0.0005825 |    0 |    0 |    0 | 0.020202 | 1e-10 | 1e-10 | 1e-10 | 1e-10 |    0 | 1e-10 |    0 | 1e-10 |    0 | 1e-10 |    0 | 1e-10 |    0 |    0 | 0           |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 | 0        |    0 |    0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 |     0 | 0.0243902 |     0 | 0      | 0.0561798 |     0 |     0 |     0 |     0 | 0         | 0         | 0         |     0 |     0 |     0 | 0.000387139 | 5.21283e-06 | 0.00124189 |     0 |     0 | 0           |     0 |     0 |     0 |     0 |     0 |     0 |     0 | 0.00267115 |
|  4 | 5.26184e-05 | 5.92741e-05 | 0.999764 | 0.34 |   0 | 0         |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 | 0.142857 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 | 0         |    0 |    0 |    0 | 0.00465382 | 0.0201735 | 0.0448802 |    0 |    0 |    0 |    0 |    0 |    0 |    0 | 0         |    0 |    0 |    0 | 0.010101 | 1e-10 | 1e-10 | 1e-10 | 1e-10 |    0 | 1e-10 |    0 | 1e-10 |    0 | 1e-10 |    0 | 1e-10 |    0 |    0 | 0           |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 | 0.116885 |    0 |    0 |     0 |     0 |     0 |     0 |     1 |     1 |     0 |     0 |     0 |     0 |     0 |     0 | 0.0243902 |     0 | 0.0625 | 0         |     0 |     0 |     0 |     0 | 0.0166667 | 0.0227273 | 0.0322581 |     0 |     0 |     0 | 0.000161733 | 5.40734e-06 | 0.00124189 |     0 |     0 | 0           |     0 |     0 |     0 |     0 |     0 |     0 |     0 | 0.00509076 |


## Standardization

### Definition

Standardization is the scaling technique in which values are shifted about mean and scaled about standard deviation. This results into 0 mean attribute and the resultant distribution has unit standard deviation.

### Formula

$$
X' = \frac{X-\mu}{\sigma}
$$

where $\mu$ is the mean value of the feature and $\sigma$ is the standard deviation of the feature.


```python
# Import StandardScaler class which will be used for standardization
from sklearn.preprocessing import StandardScaler

# Fit on training data
standardizer = StandardScaler().fit(train_df)

# Transform the tranining data
standardized_train_df = standardizer.transform(train_df)

# Transform the testing data
standardized_test_df = standardizer.transform(test_df)
```


```python
display(Markdown(pd.DataFrame(standardized_train_df).head().to_markdown()))
```


|    |        0 |        1 |         2 |          3 |          4 |         5 |          6 |          7 |         8 |          9 |        10 |         11 |   12 |   13 |      14 |        15 |        16 |          17 |        18 |        19 |        20 |          21 |        22 |         23 |         24 |          25 |         26 |         27 |   28 |   29 |   30 |   31 |        32 |         33 |         34 |         35 |          36 |        37 |       38 |   39 |         40 |   41 |   42 |         43 |         44 |          45 |         46 |        47 |         48 |   49 |   50 |   51 |   52 |        53 |   54 |   55 |   56 |        57 |         58 |        59 |         60 |          61 |        62 |        63 |   64 |   65 |         66 |          67 |   68 |         69 |   70 |        71 |         72 |         73 |          74 |         75 |         76 |         77 |          78 |        79 |        80 |          81 |          82 |         83 |   84 |   85 |         86 |         87 |         88 |          89 |          90 |         91 |         92 |          93 |   94 |          95 |   96 |         97 |          98 |          99 |   100 |        101 |   102 |        103 |       104 |       105 |   106 |   107 |        108 |        109 |        110 |       111 |       112 |       113 |       114 |       115 |       116 |   117 |   118 |       119 |       120 |       121 |       122 |   123 |   124 |   125 |       126 |        127 |        128 |       129 |        130 |        131 |        132 |   133 |         134 |         135 |        136 |        137 |        138 |         139 |
|---:|---------:|---------:|----------:|-----------:|-----------:|----------:|-----------:|-----------:|----------:|-----------:|----------:|-----------:|-----:|-----:|--------:|----------:|----------:|------------:|----------:|----------:|----------:|------------:|----------:|-----------:|-----------:|------------:|-----------:|-----------:|-----:|-----:|-----:|-----:|----------:|-----------:|-----------:|-----------:|------------:|----------:|---------:|-----:|-----------:|-----:|-----:|-----------:|-----------:|------------:|-----------:|----------:|-----------:|-----:|-----:|-----:|-----:|----------:|-----:|-----:|-----:|----------:|-----------:|----------:|-----------:|------------:|----------:|----------:|-----:|-----:|-----------:|------------:|-----:|-----------:|-----:|----------:|-----------:|-----------:|------------:|-----------:|-----------:|-----------:|------------:|----------:|----------:|------------:|------------:|-----------:|-----:|-----:|-----------:|-----------:|-----------:|------------:|------------:|-----------:|-----------:|------------:|-----:|------------:|-----:|-----------:|------------:|------------:|------:|-----------:|------:|-----------:|----------:|----------:|------:|------:|-----------:|-----------:|-----------:|----------:|----------:|----------:|----------:|----------:|----------:|------:|------:|----------:|----------:|----------:|----------:|------:|------:|------:|----------:|-----------:|-----------:|----------:|-----------:|-----------:|-----------:|------:|------------:|------------:|-----------:|-----------:|-----------:|------------:|
|  0 | -1.73203 | -1.73504 | 0.0390744 | -0.788249  | -0.0533881 | -0.213263 | -0.0382065 | -0.0134929 | -0.128231 | -0.0117701 | -0.107658 | -0.0614522 |    0 |    0 | 0.20932 | -1.40501  | -0.010259 | -0.00512928 | -0.269093 | -0.234798 | -0.101355 | -0.00512928 | -0.155643 | -0.0730034 | -0.0424901 | -0.00512928 | -0.0649149 | -0.0603643 |    0 |    0 |    0 |    0 | 0.0673233 | -0.0655252 | -0.0348091 | -0.0273928 | -0.00512928 | -0.279161 | 0.367953 |    0 | -0.0434123 |    0 |    0 | -1.18677   | -0.0225539 | -0.00567625 | -0.0312059 | -0.218677 | -0.0772892 |    0 |    0 |    0 |    0 | -0.159693 |    0 |    0 |    0 | -0.104391 | -0.0561395 | -0.125074 | -0.0243331 | -0.00808027 | -0.011058 | -0.102845 |    0 |    0 | -0.0157079 | -0.0781773  |    0 | -0.0184641 |    0 |  1.23623  | -0.0699353 | -0.0226558 | -0.00362693 | -0.0303588 | -0.0223633 | -0.0162221 | -0.00362693 | -0.007254 | -0.007254 | -0.00811027 | -0.00362693 | -0.0235115 |    0 |    0 | -0.111467  | -0.0126595 | -0.0131613 | -0.00799402 | -0.00491019 | -0.0200501 | -0.0121273 | -0.00857412 |    0 | -0.00362693 |    0 | -0.0599139 | -0.00482896 | -0.00493683 |     0 | -0.0086154 |     0 | -0.0519995 | -0.267092 | -0.385372 |     0 |     0 | -0.0146692 | -0.0187142 | -0.0238944 | -0.188906 | -0.376407 | -0.363093 | -0.266421 | -0.368213 | -0.224049 |     0 |     0 | -0.031836 | -0.177277 | -0.229084 | -0.102956 |     0 |     0 |     0 | -0.129987 | -0.0901414 | -0.0393785 | -0.018301 | -0.0660233 | -0.077435  | -0.0345129 |     0 | -0.00509493 | -0.00362693 | -0.0119793 | -0.0156501 | -0.0125762 | -0.427183   |
|  1 | -1.73198 | -1.735   | 0.0390744 |  0.0607526 | -0.0533881 | -0.213263 | -0.0382065 | -0.0134929 | -0.128231 | -0.0117701 | -0.107658 | -0.0614522 |    0 |    0 | 0.20932 | -1.40501  | -0.010259 | -0.00512928 | -0.269093 |  4.25899  | -0.101355 | -0.00512928 | -0.155643 | -0.0730034 | -0.0424901 | -0.00512928 | -0.0649149 | -0.0603643 |    0 |    0 |    0 |    0 | 0.0673233 | -0.0655252 | -0.0348091 | -0.0273928 | -0.00512928 | -0.279161 | 0.367953 |    0 | -0.0434123 |    0 |    0 | -0.0873385 | -0.0225539 | -0.00567625 | -0.0312059 | -0.218677 | -0.0772892 |    0 |    0 |    0 |    0 | -0.159693 |    0 |    0 |    0 | -0.104391 | -0.0561395 | -0.125074 | -0.0243331 | -0.00808027 | -0.011058 | -0.102845 |    0 |    0 | -0.0157079 | -0.0781773  |    0 | -0.0184641 |    0 | -0.790692 | -0.0699353 | -0.0226558 | -0.00362693 | -0.0303588 | -0.0223633 | -0.0162221 | -0.00362693 | -0.007254 | -0.007254 | -0.00811027 | -0.00362693 | -0.0235115 |    0 |    0 | -0.0996258 | -0.0126595 | -0.0131613 | -0.00799402 | -0.00491019 | -0.0200501 | -0.0121273 | -0.00857412 |    0 | -0.00362693 |    0 | -0.0599139 | -0.00482896 | -0.00493683 |     0 | -0.0086154 |     0 | -0.0519995 | -0.267092 | -0.385372 |     0 |     0 | -0.0146692 | -0.0187142 | -0.0238944 | -0.188906 | -0.376407 | -0.363093 | -0.266421 | -0.368213 | -0.224049 |     0 |     0 | -0.031836 | -0.177277 | -0.229084 | -0.102956 |     0 |     0 |     0 | -0.129987 | -0.0811522 | -0.0393785 | -0.018301 | -0.0660233 | -0.0604189 | -0.0345129 |     0 | -0.00509493 | -0.00362693 | -0.0119793 | -0.0156501 | -0.0125762 | -0.372038   |
|  2 | -1.73194 | -1.73497 | 0.0390744 | -0.788249  | -0.0533881 | -0.213263 | -0.0382065 | -0.0134929 | -0.128231 | -0.0117701 | -0.107658 | -0.0614522 |    0 |    0 | 0.20932 |  0.711737 | -0.010259 | -0.00512928 | -0.269093 | -0.234798 | -0.101355 | -0.00512928 | -0.155643 | -0.0730034 | -0.0424901 | -0.00512928 | -0.0649149 | -0.0603643 |    0 |    0 |    0 |    0 | 0.0673233 | -0.0655252 | -0.0348091 | -0.0273928 | -0.00512928 | -0.279161 | 0.367953 |    0 | -0.0434123 |    0 |    0 | -0.0873385 | -0.0225539 | -0.00567625 | -0.0312059 | -0.218677 | -0.0772892 |    0 |    0 |    0 |    0 | -0.159693 |    0 |    0 |    0 | -0.104087 | -0.0561395 | -0.125074 | -0.0243331 | -0.00808027 | -0.011058 | -0.102845 |    0 |    0 | -0.0157079 | -0.0781773  |    0 | -0.0184641 |    0 |  1.23623  | -0.0699353 | -0.0226558 | -0.00362693 | -0.0303588 | -0.0223633 | -0.0162221 | -0.00362693 | -0.007254 | -0.007254 | -0.00811027 | -0.00362693 | -0.0235115 |    0 |    0 | -0.111467  | -0.0126595 | -0.0131613 | -0.00799402 | -0.00491019 | -0.0200501 | -0.0121273 | -0.00857412 |    0 | -0.00362693 |    0 | -0.0599139 | -0.00482896 | -0.00493683 |     0 | -0.0086154 |     0 | -0.0519995 | -0.267092 | -0.385372 |     0 |     0 | -0.0146692 | -0.0187142 | -0.0238944 | -0.188906 | -0.376407 | -0.363093 | -0.266421 | -0.368213 | -0.224049 |     0 |     0 | -0.031836 | -0.177277 | -0.229084 | -0.102956 |     0 |     0 |     0 | -0.12974  | -0.0901232 | -0.0393785 | -0.018301 | -0.0660233 | -0.077435  | -0.0345129 |     0 | -0.00509493 | -0.00362693 | -0.0119793 | -0.0156501 | -0.0125762 | -0.273191   |
|  3 | -1.73189 | -1.73488 | 0.0390744 |  0.292298  | -0.0533881 |  0.361427 | -0.0382065 | -0.0134929 | -0.128231 | -0.0117701 | -0.107658 | -0.0614522 |    0 |    0 | 0.20932 |  0.711737 | -0.010259 | -0.00512928 | -0.269093 | -0.234798 | -0.101355 | -0.00512928 | -0.155643 | -0.0730034 | -0.0424901 | -0.00512928 | -0.0649149 | -0.0603643 |    0 |    0 |    0 |    0 | 0.0673233 | -0.0655252 | -0.0348091 | -0.0273928 | -0.00512928 |  3.58216  | 0.367953 |    0 | -0.0434123 |    0 |    0 |  2.11153   | -0.0225539 | -0.00567625 | -0.0312059 | -0.218677 | -0.0772892 |    0 |    0 |    0 |    0 |  3.47609  |    0 |    0 |    0 | -0.097223 | -0.0561395 | -0.125074 | -0.0243331 | -0.00808027 | -0.011058 | -0.102845 |    0 |    0 | -0.0157079 | -0.00414576 |    0 | -0.0184641 |    0 | -0.811806 | -0.0699353 | -0.0226558 | -0.00362693 | -0.0303588 | -0.0223633 | -0.0162221 | -0.00362693 | -0.007254 | -0.007254 | -0.00811027 | -0.00362693 | -0.0235115 |    0 |    0 | -0.111467  | -0.0126595 | -0.0131613 | -0.00799402 | -0.00491019 | -0.0200501 | -0.0121273 | -0.00857412 |    0 | -0.00362693 |    0 | -0.0599139 | -0.00482896 | -0.00493683 |     0 | -0.0086154 |     0 | -0.0519995 | -0.267092 | -0.385372 |     0 |     0 | -0.0146692 | -0.0187142 | -0.0238944 | -0.188906 |  0.493096 | -0.363093 | -0.266421 |  1.00412  | -0.224049 |     0 |     0 | -0.031836 | -0.177277 | -0.229084 | -0.102956 |     0 |     0 |     0 | -0.114669 | -0.0901414 | -0.0393785 | -0.018301 | -0.0660233 | -0.077435  | -0.0345129 |     0 | -0.00509493 | -0.00362693 | -0.0119793 | -0.0156501 | -0.0125762 | -0.291398   |
|  4 | -1.73185 | -1.73484 | 0.0390744 |  0.446662  | -0.0533881 | -0.213263 | -0.0382065 | -0.0134929 | -0.128231 | -0.0117701 | -0.107658 | -0.0614522 |    0 |    0 | 0.20932 | -1.40501  | -0.010259 | -0.00512928 |  3.71619  | -0.234798 | -0.101355 | -0.00512928 | -0.155643 | -0.0730034 | -0.0424901 | -0.00512928 | -0.0649149 | -0.0603643 |    0 |    0 |    0 |    0 | 0.0673233 | -0.0655252 | -0.0348091 | -0.0273928 | -0.00512928 | -0.279161 | 0.367953 |    0 | -0.0434123 |    0 |    0 | -0.0873385 | -0.0225539 | -0.00567625 | -0.0312059 | -0.218677 | -0.0772892 |    0 |    0 |    0 |    0 | -0.159693 |    0 |    0 |    0 | -0.104391 | -0.0561395 |  2.67905  | -0.0243331 | -0.00808027 | -0.011058 | -0.102845 |    0 |    0 | -0.0157079 | -0.0781773  |    0 | -0.0184641 |    0 | -0.83292  | -0.0699353 | -0.0226558 | -0.00362693 | -0.0303588 | -0.0223633 | -0.0162221 | -0.00362693 | -0.007254 | -0.007254 | -0.00811027 | -0.00362693 | -0.0235115 |    0 |    0 | -0.111467  | -0.0126595 | -0.0131613 | -0.00799402 | -0.00491019 | -0.0200501 | -0.0121273 | -0.00857412 |    0 | -0.00362693 |    0 |  9.41013   | -0.00482896 | -0.00493683 |     0 | -0.0086154 |     0 | -0.0519995 |  3.74403  |  2.59489  |     0 |     0 | -0.0146692 | -0.0187142 | -0.0238944 | -0.188906 |  0.493096 | -0.363093 |  2.58475  | -0.368213 | -0.224049 |     0 |     0 | -0.031836 |  1.17661  |  1.45743  |  2.45646  |     0 |     0 |     0 | -0.12974  | -0.090111  | -0.0393785 | -0.018301 | -0.0660233 | -0.077435  | -0.0345129 |     0 | -0.00509493 | -0.00362693 | -0.0119793 | -0.0156501 | -0.0125762 |  0.00041152 |


!!! Note
    StandardScaler should only be used for numerical features.
    
!!! Note
    Both MinMaxScaler and StandardScaler are sensitive to outliers.

## Normalization vs Standardization

There is not hard and fast rule about when to use normalization and when to use standardization. Whichever methode gives best performance after fitting the data to machine learning model should be chosen. But generally, normalization is used when we know that data does not follow Gaussian distribution and standardization is used when we know that data follow Gaussian distribution *(take this sentence with a pinch of salt)*.

## References

- [Feature Scaling for Machine Learning: Understanding the Difference Between Normalization vs. Standardization](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/)

- [Compare the effect of different scalers on data with outliers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)


